% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{heavylasso}
\alias{heavylasso}
\title{Heavy-tailed Lasso Regression using an EM Algorithm}
\usage{
heavylasso(
  X,
  y,
  lambda,
  nu = 3,
  sigma2 = 1,
  max_iter = 2000,
  tol = 1e-11,
  beta_init = NULL
)
}
\arguments{
\item{X}{A numeric matrix of predictors of dimension \eqn{n \times p}.}

\item{y}{A numeric response vector of length \eqn{n}.}

\item{lambda}{Non-negative regularization parameter controlling sparsity.}

\item{nu}{Degrees of freedom for the Student-\eqn{t} distribution (default: 3).}

\item{sigma2}{Fixed error variance, assumed known (default: 1).}

\item{max_iter}{Maximum number of EM iterations (default: 2000).}

\item{tol}{Convergence threshold based on mean squared difference of coefficients (default: 1e-11).}

\item{beta_init}{Optional numeric vector of initial coefficient values. If \code{NULL}, initialized to zero.}
}
\value{
A list with the following components:
\describe{
\item{coefficients}{Estimated regression coefficients \eqn{\hat{\beta}}.}
\item{weights}{Observation-specific weights derived from the E-step.}
\item{iterations}{Number of iterations completed before convergence.}
}
}
\description{
Fits a robust linear regression model under a heavy-tailed error assumption
using an EM-like algorithm. The model incorporates \eqn{\ell_1}-penalization
(lasso) and assumes a Student-t distribution for the errors.
}
\details{
This function solves the following penalized regression problem under heavy-tailed errors:

\deqn{ \min_{\beta} \sum_{i=1}^{n} w_i (y_i - x_i^\top \beta)^2 + \lambda \|\beta\|_1, }

where the weights \eqn{w_i} are updated in the E-step assuming a Student-\eqn{t} likelihood:

\deqn{ w_i = \frac{\nu + 1}{\nu + r_i^2 / \sigma^2} / 2, }

and \eqn{r_i = y_i - x_i^\top \beta} is the residual for observation \eqn{i}. This arises from
modeling the errors with a scale-mixture of normals, which is equivalent to assuming:

\deqn{ y_i = x_i^\top \beta + \epsilon_i, \quad \epsilon_i \sim t_\nu(0, \sigma^2). }

The algorithm alternates between:
\itemize{
\item \strong{E-step}: Update observation-specific weights \eqn{w_i}.
\item \strong{M-step}: Solve a weighted lasso problem via coordinate descent.
}
}
\section{Algorithm}{

\enumerate{
\item Initialize \eqn{\beta^{(0)}}, set all weights \eqn{w_i = 1}.
\item Repeat until convergence or maximum iterations:
\enumerate{
\item E-step: Compute residuals and update weights as above.
\item M-step: Solve the weighted lasso problem using coordinate descent.
\item Check for convergence based on squared difference in coefficients.
}
}
}

\examples{
set.seed(123)
n <- 100; p <- 10
X <- matrix(rnorm(n * p), n, p)
beta_true <- c(1, -1, rep(0, p - 2))
y <- X \%*\% beta_true + rt(n, df = 3)  # heavy-tailed noise
fit <- heavylasso(X, y, lambda = 0.1)
print(fit$coefficients)

}
\seealso{
\code{\link{select_lambda_ic}} for automated lambda selection.
}
