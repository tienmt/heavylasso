% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{cv_heavylasso}
\alias{cv_heavylasso}
\title{Cross-validation Heavy-tailed Lasso Regression: Student loss, also known as HeavyLasso}
\arguments{
\item{X}{A numeric matrix of predictors of dimension \eqn{n \times p}.}

\item{y}{A numeric response vector of length \eqn{n}.}

\item{lambda}{Non-negative regularization parameter controlling sparsity.}

\item{nu}{Robustness or temperature parameter (default: 3). The author recommend IQR(y)*4.}

\item{sigma2}{Fixed error variance, assumed known (default: 1).}

\item{max_iter}{Maximum number of EM iterations (default: 2000).}

\item{tol}{Convergence threshold based on mean squared difference of coefficients (default: 1e-11).}

\item{beta_init}{Optional numeric vector of initial coefficient values. If \code{NULL}, initialized to zero.}
}
\value{
A list with the following components:
\describe{
\item{coefficients}{Estimated regression coefficients \eqn{\hat{\beta}}.}
\item{weights}{Observation-specific weights derived from the E-step.}
\item{iterations}{Number of iterations completed before convergence.}
}
}
\description{
Fits a robust linear regression model under a heavy-tailed error assumption
using an EM-like algorithm. The model incorporates \eqn{\ell_1}-penalization
(lasso) and assumes a Student-t distribution for the errors.
}
\details{
This function solves the following penalized regression problem under heavy-tailed errors:

\deqn{ \min_{\beta} \sum_{i=1}^{n} w_i (y_i - x_i^\top \beta)^2 + \lambda \|\beta\|_1, }

where the weights \eqn{w_i} are updated in the E-step assuming a Student-\eqn{t} likelihood:

\deqn{ w_i = \frac{\nu + 1}{\nu + r_i^2 / \sigma^2} / 2, }

and \eqn{r_i = y_i - x_i^\top \beta} is the residual for observation \eqn{i}. This arises from
modeling the errors with a scale-mixture of normals, which is equivalent to assuming:

\deqn{ y_i = x_i^\top \beta + \epsilon_i, \quad \epsilon_i \sim t_\nu(0, \sigma^2). }

The algorithm alternates between:
\itemize{
\item \strong{E-step}: Update observation-specific weights \eqn{w_i}.
\item \strong{M-step}: Solve a weighted lasso problem via coordinate descent.
}
}
\section{Algorithm}{

\enumerate{
\item Initialize \eqn{\beta^{(0)}}, set all weights \eqn{w_i = 1}.
\item Repeat until convergence or maximum iterations:
\enumerate{
\item E-step: Compute residuals and update weights as above.
\item M-step: Solve the weighted lasso problem using coordinate descent.
\item Check for convergence based on squared difference in coefficients.
}
}
}

\examples{
n_test = 10000
n <- n_test + 100
p <- 120
s0 = 10
beta_true <- rep(0, p) ;
beta_true[1:s0] <- c( rep(1, s0/2) , rep(-1, s0/2) )
X1 <- matrix(rnorm(n * p), n, p)

y1 <- X1 \%*\% beta_true + rt(n, df = 30)
# rnorm(n,sd=3)  # rnorm(n)  # rcauchy(n) # rt(n, df = 3) #

y = y1[-(1:n_test),]
X = X1[-(1:n_test),]
ytest = y1[ 1:n_test,] ;
xtest = X1[1:n_test,]

cv.heavylasso1 <- cv_heavylasso(X, y, nu = IQR(y)*3,lambdas = 1:50 )
b_heavyt2 <- cv.heavylasso1$beta_best
cv.explasso1 <- cv_expLasso(X, y, tau = 0.1, lambda = 1:50 )
b_explasso <- cv.explasso1$beta_best

sum( (b_explasso - beta_true)^2)
sum( (b_heavyt2 - beta_true)^2)

}
\references{
Mai, T. T. (2025). Heavy Lasso: sparse penalized regression under heavy-tailed noise via data-augmented soft-thresholding. arXiv preprint arXiv:2506.07790.

Mai, T. T. (2025). Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss.
}
\seealso{
\code{\link{select_lambda_ic}} for automated lambda selection.
}
\author{
The Tien Mai (\href{mailto:the.tien.mai@fhi.no}{the.tien.mai@fhi.no})
}
